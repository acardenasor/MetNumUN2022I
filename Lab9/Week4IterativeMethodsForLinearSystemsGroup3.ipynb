{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acardenasor/MetNumUN2022I/blob/main/Week4IterativeMethodsForLinearSystemsGroup3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7zSSeX21Grj"
      },
      "source": [
        "# Simple iteration for systems of linear equations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRz8VJ0N1Grn"
      },
      "source": [
        "First, generate a random diagonally dominant matrix, for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "collapsed": true,
        "id": "jTVkb-WZ1Gro"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "rndm = np.random.RandomState(1234)\n",
        "\n",
        "n = 10\n",
        "A = rndm.uniform(size=(n, n)) + np.diagflat([15]*n)\n",
        "b = rndm.uniform(size=n)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A"
      ],
      "metadata": {
        "id": "4mv5q-nMKZwY",
        "outputId": "199f1f81-9be9-4e06-8558-8eafdefdcf3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[15.192,  0.622,  0.438,  0.785,  0.78 ,  0.273,  0.276,  0.802,\n",
              "         0.958,  0.876],\n",
              "       [ 0.358, 15.501,  0.683,  0.713,  0.37 ,  0.561,  0.503,  0.014,\n",
              "         0.773,  0.883],\n",
              "       [ 0.365,  0.615, 15.075,  0.369,  0.933,  0.651,  0.397,  0.789,\n",
              "         0.317,  0.568],\n",
              "       [ 0.869,  0.436,  0.802, 15.144,  0.704,  0.705,  0.219,  0.925,\n",
              "         0.442,  0.909],\n",
              "       [ 0.06 ,  0.184,  0.047,  0.675, 15.595,  0.533,  0.043,  0.561,\n",
              "         0.33 ,  0.503],\n",
              "       [ 0.112,  0.607,  0.566,  0.007,  0.617, 15.912,  0.791,  0.992,\n",
              "         0.959,  0.792],\n",
              "       [ 0.285,  0.625,  0.478,  0.196,  0.382,  0.054, 15.452,  0.982,\n",
              "         0.124,  0.119],\n",
              "       [ 0.739,  0.587,  0.472,  0.107,  0.229,  0.9  ,  0.417, 15.536,\n",
              "         0.006,  0.301],\n",
              "       [ 0.437,  0.612,  0.918,  0.626,  0.706,  0.15 ,  0.746,  0.831,\n",
              "        15.634,  0.438],\n",
              "       [ 0.153,  0.568,  0.528,  0.951,  0.48 ,  0.503,  0.537,  0.819,\n",
              "         0.057, 15.669]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNGIGBex1Grq"
      },
      "source": [
        "# I.  Jacobi iteration\n",
        "\n",
        "Given\n",
        "\n",
        "$$\n",
        "A x = b\n",
        "$$\n",
        "\n",
        "separate the diagonal part $D$,\n",
        "\n",
        "$$ A = D + (A - D) $$\n",
        "\n",
        "and write\n",
        "\n",
        "$$\n",
        "x = D^{-1} (D - A) x + D^{-1} b\\;.\n",
        "$$\n",
        "\n",
        "Then iterate\n",
        "\n",
        "$$\n",
        "x_{n + 1} = B x_{n} + c\\;,\n",
        "$$\n",
        "\n",
        "where \n",
        "\n",
        "$$\n",
        "B = D^{-1} (A - D) \\qquad \\text{and} \\qquad c = D^{-1} b\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rB2PHHIK1Grr"
      },
      "source": [
        "Let's construct the matrix and the r.h.s. for the Jacobi iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "bnk796g61Grr"
      },
      "outputs": [],
      "source": [
        "diag_1d = np.diag(A)\n",
        "\n",
        "B = -A.copy()\n",
        "np.fill_diagonal(B, 0)\n",
        "\n",
        "D = np.diag(diag_1d)\n",
        "invD = np.diag(1./diag_1d)\n",
        "BB = invD @ B \n",
        "c = invD @ b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "0NTUyC3i1Grs"
      },
      "outputs": [],
      "source": [
        "# sanity checks\n",
        "from numpy.testing import assert_allclose\n",
        "\n",
        "assert_allclose(-B + D, A)\n",
        "\n",
        "\n",
        "# xx is a \"ground truth\" solution, compute it using a direct method\n",
        "xx = np.linalg.solve(A, b)\n",
        "\n",
        "np.testing.assert_allclose(A@xx, b)\n",
        "np.testing.assert_allclose(D@xx, B@xx + b)\n",
        "np.testing.assert_allclose(xx, BB@xx + c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRaVUE6U1Grt"
      },
      "source": [
        "Check that $\\| B\\| \\leqslant 1$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "zgED2OGu1Gru",
        "outputId": "92105d81-3cab-4256-b0f8-5a1487975298",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.36436161983015336"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "np.linalg.norm(BB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlBcusqg1Grv"
      },
      "source": [
        "### Do the Jacobi iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "collapsed": true,
        "id": "NK6iwDtm1Grv"
      },
      "outputs": [],
      "source": [
        "n_iter = 50\n",
        "\n",
        "x0 = np.ones(n)\n",
        "x = x0\n",
        "for _ in range(n_iter):\n",
        "    x = BB @ x + c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "gsLyio7W1Grw",
        "outputId": "bed81d5f-f0e9-4871-fad9-d59e084a2db0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.,  0., -0., -0.,  0.,  0., -0.,  0., -0.,  0.])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "# Check the result:\n",
        "\n",
        "A @ x - b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXfb9BuT1Grw"
      },
      "source": [
        "### Task I.1\n",
        "\n",
        "Collect the proof-of-concept above into a single function implementing the Jacobi iteration. This function should receive the r.h.s. matrix $A$, the l.h.s. vector `b`, and the number of iterations to perform.\n",
        "\n",
        "\n",
        "The matrix $A$ in the illustration above is strongly diagonally dominant, by construction. \n",
        "What happens if the diagonal matrix elements of $A$ are made smaller? Check the convergence of the Jacobi iteration, and check the value of the norm of $B$.\n",
        "\n",
        "(20% of the total grade)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "collapsed": true,
        "id": "5zqTUVVa1Grw"
      },
      "outputs": [],
      "source": [
        "def jacobi(A, b, num_it=1000):\n",
        "    diag_A = np.diag(A)    \n",
        "    BB = -A.copy()             \n",
        "    np.fill_diagonal(BB, 0)    \n",
        "    D = np.diag(diag_A)        \n",
        "    invD = np.diag(1./diag_A)\n",
        "    B = invD @ BB             \n",
        "    c = invD @ b\n",
        "    norm = np.linalg.norm(B)\n",
        "    \n",
        "    x0 = np.ones(A.shape[0])\n",
        "    x = x0\n",
        "    for _ in range(num_it):\n",
        "        x = B @ x + c\n",
        "    \n",
        "    return x, norm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jacobi(A, b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjbQKI8iFhrE",
        "outputId": "d9c89164-37c7-432a-eb91-db8b0345b935"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 0.039,  0.038,  0.043,  0.024,  0.057, -0.   , -0.006,  0.032,\n",
              "        -0.004,  0.053]), 0.36436161983015336)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rndm = np.random.RandomState(1234)\n",
        "\n",
        "n = 10\n",
        "A = rndm.uniform(size=(n, n)) + np.diagflat([15] * n)    # CHANGE THIS\n",
        "b = rndm.uniform(size=n)\n",
        "\n",
        "print(A)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GiULNXlF4yL",
        "outputId": "4eacc036-76e1-4532-f78e-fdae90060671"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[15.192  0.622  0.438  0.785  0.78   0.273  0.276  0.802  0.958  0.876]\n",
            " [ 0.358 15.501  0.683  0.713  0.37   0.561  0.503  0.014  0.773  0.883]\n",
            " [ 0.365  0.615 15.075  0.369  0.933  0.651  0.397  0.789  0.317  0.568]\n",
            " [ 0.869  0.436  0.802 15.144  0.704  0.705  0.219  0.925  0.442  0.909]\n",
            " [ 0.06   0.184  0.047  0.675 15.595  0.533  0.043  0.561  0.33   0.503]\n",
            " [ 0.112  0.607  0.566  0.007  0.617 15.912  0.791  0.992  0.959  0.792]\n",
            " [ 0.285  0.625  0.478  0.196  0.382  0.054 15.452  0.982  0.124  0.119]\n",
            " [ 0.739  0.587  0.472  0.107  0.229  0.9    0.417 15.536  0.006  0.301]\n",
            " [ 0.437  0.612  0.918  0.626  0.706  0.15   0.746  0.831 15.634  0.438]\n",
            " [ 0.153  0.568  0.528  0.951  0.48   0.503  0.537  0.819  0.057 15.669]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jacobi(A, b, 5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEcJm76FF_00",
        "outputId": "df1ea149-d49a-4053-e6f8-ef86f5d5b767"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 0.039,  0.038,  0.043,  0.024,  0.057, -0.   , -0.006,  0.032,\n",
              "        -0.004,  0.053]), 0.36436161983015336)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBnguRFT1Grx"
      },
      "source": [
        "# II. Seidel's iteration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSbuFdYU1Grx"
      },
      "source": [
        "##### Task II.1\n",
        "\n",
        "Implement the Seidel's iteration. \n",
        "\n",
        "Test it on a random matrix. Study the convergence of iterations, relate to the norm of the iteration matrix.\n",
        "\n",
        "(30% of the total grade)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "collapsed": true,
        "id": "FYygs3Ml1Grx"
      },
      "outputs": [],
      "source": [
        "def seidel(A, b, n_iter=1000):\n",
        "    diag_A = np.diag(A)        \n",
        "    D = np.diag(diag_A)        \n",
        "    invD = np.diag(1./diag_A)\n",
        "    U = np.triu(A) - D\n",
        "    L = np.tril(A) - D\n",
        "    B_tilde = -np.linalg.inv(D+L) @ U \n",
        "    c_tilde = np.linalg.inv(D+L) @ b \n",
        "    \n",
        "    norm = np.linalg.norm(B)\n",
        "    \n",
        "    x0 = np.ones(A.shape[0])\n",
        "    x = x0\n",
        "    \n",
        "    for _ in range(n_iter):\n",
        "        x = B_tilde @ x + c_tilde\n",
        "    \n",
        "    return x, norm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.set_printoptions(precision=3, suppress=True)\n",
        "\n",
        "iters = range(1,15)\n",
        "\n",
        "for i in iters:\n",
        "    print('Jacobi', jacobi(A, b, i)[0])\n",
        "    print('Seidel', seidel(A, b, i)[0])\n",
        "    print('')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdLAx-pJGZla",
        "outputId": "a89ba395-42f4-475a-f155-0302a7112825"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jacobi [-0.332 -0.268 -0.279 -0.36  -0.126 -0.333 -0.208 -0.204 -0.342 -0.233]\n",
            "Seidel [-0.332 -0.237 -0.196 -0.185 -0.052 -0.192 -0.051  0.064  0.02   0.096]\n",
            "\n",
            "Jacobi [0.15  0.135 0.135 0.139 0.117 0.092 0.054 0.106 0.094 0.138]\n",
            "Seidel [ 0.072  0.064  0.058  0.03   0.06  -0.006 -0.011  0.028 -0.008  0.051]\n",
            "\n",
            "Jacobi [ 0.006  0.009  0.015 -0.011  0.04  -0.028 -0.024  0.01  -0.034  0.027]\n",
            "Seidel [ 0.038  0.037  0.043  0.024  0.058  0.    -0.005  0.032 -0.004  0.053]\n",
            "\n",
            "Jacobi [ 0.049  0.046  0.051  0.034  0.063  0.008 -0.     0.038  0.005  0.061]\n",
            "Seidel [ 0.039  0.038  0.043  0.024  0.057 -0.    -0.006  0.032 -0.004  0.053]\n",
            "\n",
            "Jacobi [ 0.036  0.035  0.04   0.021  0.056 -0.003 -0.007  0.03  -0.007  0.051]\n",
            "Seidel [ 0.039  0.038  0.043  0.024  0.057 -0.    -0.006  0.032 -0.004  0.053]\n",
            "\n",
            "Jacobi [ 0.04   0.039  0.044  0.025  0.058  0.    -0.005  0.032 -0.003  0.054]\n",
            "Seidel [ 0.039  0.038  0.043  0.024  0.057 -0.    -0.006  0.032 -0.004  0.053]\n",
            "\n",
            "Jacobi [ 0.039  0.038  0.043  0.023  0.057 -0.001 -0.006  0.032 -0.004  0.053]\n",
            "Seidel [ 0.039  0.038  0.043  0.024  0.057 -0.    -0.006  0.032 -0.004  0.053]\n",
            "\n",
            "Jacobi [ 0.039  0.038  0.043  0.024  0.057 -0.    -0.006  0.032 -0.004  0.053]\n",
            "Seidel [ 0.039  0.038  0.043  0.024  0.057 -0.    -0.006  0.032 -0.004  0.053]\n",
            "\n",
            "Jacobi [ 0.039  0.038  0.043  0.024  0.057 -0.    -0.006  0.032 -0.004  0.053]\n",
            "Seidel [ 0.039  0.038  0.043  0.024  0.057 -0.    -0.006  0.032 -0.004  0.053]\n",
            "\n",
            "Jacobi [ 0.039  0.038  0.043  0.024  0.057 -0.    -0.006  0.032 -0.004  0.053]\n",
            "Seidel [ 0.039  0.038  0.043  0.024  0.057 -0.    -0.006  0.032 -0.004  0.053]\n",
            "\n",
            "Jacobi [ 0.039  0.038  0.043  0.024  0.057 -0.    -0.006  0.032 -0.004  0.053]\n",
            "Seidel [ 0.039  0.038  0.043  0.024  0.057 -0.    -0.006  0.032 -0.004  0.053]\n",
            "\n",
            "Jacobi [ 0.039  0.038  0.043  0.024  0.057 -0.    -0.006  0.032 -0.004  0.053]\n",
            "Seidel [ 0.039  0.038  0.043  0.024  0.057 -0.    -0.006  0.032 -0.004  0.053]\n",
            "\n",
            "Jacobi [ 0.039  0.038  0.043  0.024  0.057 -0.    -0.006  0.032 -0.004  0.053]\n",
            "Seidel [ 0.039  0.038  0.043  0.024  0.057 -0.    -0.006  0.032 -0.004  0.053]\n",
            "\n",
            "Jacobi [ 0.039  0.038  0.043  0.024  0.057 -0.    -0.006  0.032 -0.004  0.053]\n",
            "Seidel [ 0.039  0.038  0.043  0.024  0.057 -0.    -0.006  0.032 -0.004  0.053]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3PDxY8W1Gry"
      },
      "source": [
        "# III. Minimum residual scheme"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbfUD6zy1Gry"
      },
      "source": [
        "### Task III.1\n",
        "\n",
        "Implement the $\\textit{minimum residual}$ scheme: an explicit non-stationary method, where at each step you select the iteration parameter $\\tau_n$ to minimize the residual $\\mathbf{r}_{n+1}$ given $\\mathbf{r}_n$. Test it on a random matrix, study the convergence to the solution, in terms of the norm of the residual and the deviation from the ground truth solution (which you can obtain using a direct method). Study how the iteration parameter $\\tau_n$ changes as iterations progress.\n",
        "\n",
        "(50% of the grade)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "collapsed": true,
        "id": "60_IGETi1Gry"
      },
      "outputs": [],
      "source": [
        "def minres(A, b, n_iter=500):\n",
        "    x = np.ones(b.shape[0])\n",
        "    T = []\n",
        "    for _ in range(n_iter):\n",
        "        r = A @ x - b\n",
        "        tau = (r @ A @ r)/np.linalg.norm(A @ r)**2\n",
        "        x = x - tau*r\n",
        "        T.append(tau)\n",
        "    return x, T\n",
        "\n",
        "x = minres(A, b)[0]\n",
        "\n",
        "np.testing.assert_allclose(A@x, b)\n",
        "np.testing.assert_allclose(x, xx)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "minres(A, b)[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJgtpewNGpFT",
        "outputId": "dbc8cda4-14ae-4ae2-8d1c-bcef0c935332"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.049458091915919704,\n",
              " 0.06632815899696641,\n",
              " 0.056465283719342306,\n",
              " 0.05913686144833426,\n",
              " 0.056149365850670686,\n",
              " 0.05962056630692138,\n",
              " 0.05532125962259336,\n",
              " 0.06019139542967885,\n",
              " 0.054726519624928915,\n",
              " 0.060918354718292295,\n",
              " 0.05478879426160098,\n",
              " 0.061543172821534486,\n",
              " 0.055177211118735095,\n",
              " 0.06162125672274042,\n",
              " 0.055145295102092665,\n",
              " 0.0617011494677153,\n",
              " 0.05472584159792917,\n",
              " 0.062121454652208655,\n",
              " 0.054002133608905656,\n",
              " 0.0644186170983185,\n",
              " 0.05443303564328217,\n",
              " 0.06011645923749506,\n",
              " 0.06271150857694172,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "Week4IterativeMethodsForLinearSystemsGroup3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
